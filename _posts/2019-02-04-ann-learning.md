---
bg: "deep.jpg"
layout: post
title:  "3장, 신경망 학습"
crawlertitle: "Artificial Neural Networks"
summary: "ANN_Learning"
date:   2019-02-04
categories: posts
tags: 'deep_learning'
author: parkjunsoo
---

#### 학습이란?

훈련 데이터로 부터 가중치 매개변수(W, b)의 최적값을 찾는 것이다.  

이 장에서는,  
신경망이 학습할 수 있도록 해주는 지표인 손실함수를 소개하며,  
손실 함수의 결과값(오차)이 최소인 곳을 찾는 경사(하강)법을 공부한다.

***

#### 훈련 데이터(training data)와 시험 데이터(test data)

보통 기계학습 문제는 데이터를 훈련/시험 데이터로 나누어 학습/평가한다.

훈련 데이터만을 사용하여 최적의 매개변수를 찾는다.

그 후, 시험 데이터를 사용하여 훈련한 모델을 평가한다.

##### 오버피팅(OVERFITTING)
데이터 셋(data set) 1개에만 지나치게 최적화된 상태를 오버피팅이라 한다.

한 쪽 이야기만 너무 많이 들어서 편견이 생긴 상태로 볼 수 있다.

오버피팅을 피하기 위해서 훈련 데이터, 시험 데이터로 나눈다.

***

#### 손실 함수(loss function) // 비용 함수(cost function)

손실 함수는 신경망이 현재 훈련 데이터를 잘 처리하는지, 못 하는지를 나타내는 지표로 사용한다.

즉, 오차를 계산하는 함수이다.

##### 가장 많이 쓰이는 손실 함수로
##### 평균 제곱 오차(Mean Square Error, MSE)
##### 평균 제곱근 오차(Root Mean Square Error, RMSE)가 있다.

![rmse](https://github.com/junsoofeb/junsoofeb.github.io/raw/master/assets/images/rmse.png)

y는 실제 값, y_hat은 예측 값이다.

위의 식은 Root를 씌운 MSE로 RMSE이다.  
MSE는 루트만 제거하면 된다.

MSE 값이 너무 커서 계산이 오래걸릴 때,  RMSE를 사용한다.

##### 크로스 엔트로피 오차(Cross Entropy Error, CEE)도 자주 쓰이는 손실 함수이다.

![cee](https://github.com/junsoofeb/junsoofeb.github.io/raw/master/assets/images/cee.png)

p는 실제 값, q는 예측 값이다.

##### MSE와 CEE 계산 결과가 작을수록 오차가 작은 것이다.

***

#### 미니 배치(mini batch) 학습

기계 학습은 훈련 데이터에 대한 손실 함수의 값(오차)을 구하고, 그 값을 최소로 만드는 매개변수를 찾는다.

이 방식은 모든 훈련 데이터를 대상으로 손실 함수의 값을 구해야 한다.

즉, 평균 손실 함수의 값을 구한다.  
N개의 데이터가 있다면, N개의 데이터에서 손실 함수의 값을 모두 더하고, 마지막에 N으로 나눈다.

ex) 훈련 데이터가 100개면, 그로부터 계산한 100개의 손실 함수의 값들의 평균이 오차가 된다.

##### 그런데 데이터의 수 N이 너무 커지면 이 방법은 현실적이지 않다.

이런 경우, 데이터의 일부를 골라서 전체 데이터의 근사값으로 이용하는 방법이 적합하다.

훈련 데이터 중에서 일부만 골라 학습하는 방법을 미니 배치 학습이다.

***

#### 왜 손실 함수를 사용하는가?

우리의 목표는 높은 [정확도]를 갖는 매개변수를 찾는 것이다.  
정확도라는 지표를 두고 굳이 중간에 손실 함수를 사용하는 것은 빙~ 돌아서 문제를 푸는게 아닐까?

##### 신경망 학습에서의 [미분]의 역할에 주목해보자.

결론부터 말하면, [정확도]를 지표로 삼으면 안된다.  
정확도를 지표로 하면 매개변수의 미분이 대부분의 위치에서 0이 되기 때문이다.

왜 정확도를 기준으로 하면 매개변수의 미분이 0인가?

ex) 100개의 훈련 데이터중 42개를 맞춘다면, 그 신경망의 정확도는 42%이다.  

정확도가 기준일 때는 훈련 데이터를 정확하게 맞춰야만 정확도가 올라가기 때문에, 매개변수를 조금 조절해서는 정확도가 변하지 않는다.  

우연히 매개변수를 약간 갱신해서 훈련 데이터를 맞췄다고 해도 43%, 44%처럼 불연속하게 바뀐다.

손실함수를 사용한다면, 현재 오차는 0.9254----와 같이 연속적인 수치로 나타낼 수 있고, 매개변수를 갱신함에 따라서 0.9342----처럼 연속적으로 변화하는 것을 확인할 수 있다.

이 문제는 신경망 학습에서 [계단 함수]를 활성화 함수로 사용하지 않는 것과도 같은 이유이다.

신경망 학습에서는 시그모이드, 렐루 함수 등을 활성화 함수로 주로 사용한다.

***

#### 미분(수치미분, 편미분)과 기울기

미분은 한 순간의 변화량이다.  

f(x)의 x에 대한 미분의 의미는 [x의 작은 변화가 f(x)를 얼마나 변화시키는가?] 하는 뜻이다.

수치 미분 : 아주 작은 차분으로 미분하는 것이다. ( 보통의 미분)

차분 : 임의의 두 점에서 함수 값들의 차이.

###### 수치미분에는 미세한 오차가 있긴 하다. 진정한 접선이 아니다는 뜻.
###### limit h->0 일 때 f(x + h) - f(x) / f(h) 에서 h를 무한히 0으로 좁히는 것이 불가능하기 때문이다.

편미분 : 변수가 여럿인 함수에 대한 미분이다. 여러개의 변수 중 미분할 변수를 정하고, 나머지는 상수 취급한다.

기울기는 모든 변수의 편미분을 벡터로 정리한 것이다.

벡터 : 크기와 방향을 가진 물리량이다.

***

#### 경사법

경사법 : 기울기를 활용하여 함수의 최댓값 또는 최솟값(또는 가능한 한 큰 값, 작은 값)을 찾는 방법이다.

주의할 점은 기울기가 제시하는 방향에 정말 함수의 최댓값 또는 최솟값이 있는지 보장할 수 있다는 점이다.

ex) 안장점 : 어느 방향에서는 극댓값, 다른 방향에서는 극솟값

기울기가 0이지만 최댓값, 최솟값은 아닌 지점이다.

그래도 기울기가 가리키는 방향으로 가면 함수의 값이 증가 또는 감소 하는 것은 확실하다.

경사법 매카니즘

1. 현 위치에서 기울어진 방향으로 일정 거리 이동
2. 이동한 위치에서 기울기 구하고, 그 방향으로 다시 일정거리 이동
3. 기울기 0이 될 때 까지 반복.

##### 경사법으로 최댓값을 찾는 것 : 경사상승법
##### 경사법으로 최솟값을 찾는 것 : 경사하강법

둘 다 본질은 같다. 손실 함수의 부호만 바뀔 뿐.

***
#### 학습률 (learning rate)

경사법에서 얼마만큼의 거리를 이동해야 할 지를 학습률이라고 한다.

즉, 매개변수 값을 얼마나 갱신하느냐를 결정한다.

학습률이 너무 크면, 매개변수 값이 큰 값으로 발산하며  
학습률이 너무 작으면, 매개변수 값이 거의 갱신되지 않는다.

학습률 같은 매개변수를 하이퍼 파라미터(hyper parameter)라고 한다.   
사람이 직접 설정해야하는 매개변수이다.

W, b와 같은 가중치 매개변수는 학습에 의해 자동으로 설정되는 매개변수라서 하이퍼 파라미터가 아니다.

***

#### 확률적 경사하강법(SGD)을 이용한 학습 알고리즘

1. 미니 배치(mini batch)  
훈련 데이터 중 일부를 무작위로 고른다.  
그 미니 배치의 손실 함수 값을 최소로 하는 것이 목표이다.

2. 기울기 구하기  
미니 배치의 손실 함수 값을 줄이기 위해, 각 가중치 매개변수의 기울기를 구한다.  
기울기가 오차를 줄여주는 방향을 제시한다.

3. 매개변수 갱신  
가중치 매개변수를 기울기가 제시한 방향으로 학습률에 따라 갱신한다.

4. [ 1 ~ 3 단계] 반복한다.

이 알고리즘은 미니 배치를 사용하기 때문에,  
확률적 경사 하강법(Stochastic Gradient Descent, SGD)이라 한다.

그 후 시험 데이터로 평가하는 과정을 거친다.  
오버피팅이 일어나는지 여부 판단.

##### cf) 에폭(epoch)  

에폭은 단위이다.

1에폭은 학습에서 훈련 데이터를 모두 소진했을 때의 횟수이다.

10000개의 훈련 데이터를 100개의 미니 배치로 학습했다면,  
_SGD 100회 반복하면 모든 훈련 데이터를 소진하게 된다._

이런 경우 _100회가 1에폭_ 이 된다.
