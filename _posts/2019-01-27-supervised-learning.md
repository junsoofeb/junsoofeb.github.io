---
layout: post
title:  "지도 학습"
crawlertitle: "supervised learning"
summary: "지도 학습"
date:   2019-01-27
categories: posts
tags: 'supervised_learning'
author: parkjunsoo
---

#### 지도 학습(Supervised Learning) 기초

지도 학습이란 입력 데이터 집합과 그 집합의 각 예제 샘플에 대한 실제값을 가지고 추론 모델(inference model)을 학습시키는 것을 말한다.  

##### 쉽게 말해 답이 정해져 있는 데이터(labeled examples, training data set)를 가지고 학습하는 것이다.  

_추론 모델은 데이터에 적용되는 일련의 수학적 오퍼레이션_ 이다.  
우리의 모델을 구성하는 오퍼레이션은 고정되어 있다.  
각 오퍼레이션 내부에는 "multiply 2", "add 3" 과 같은 임의의 값들이 있는데, 이러한 값들을 그 _모델의 파라미터_ 라고 하며,  
학습 과정을 거치며 변경되는 값이다.  


추론 모델은 사용되는 오퍼레이션 개수, 결합 방식, 사용되는 파라미터의 개수 등에 따라 다양하게 변경될 수 있지만  
학습을 위해서는 언제나 동일한 일반적인 구조를 적용한다.    


[Training loop]


1. 처음에 모델 파라미터를 초기화한다.  
   대부분 랜덤값을 사용하며 단순한 모델인 경우 0으로 초기화하기도 한다.    

2. 학습 데이터(training data set)를 입력한다.  
   대부분의 오퍼레이션은 데이터의 순서를 랜덤하게 바꾸어서 모델에 입력되는 데이터의 순서를 다르게 한다.  
    
3. 학습 데이터를 이용해서 추론 모델을 실행한다.  
   이 단계에서 현재의 모델 파라미터를 이용해서 각 학습 입력 데이터에 대한 출력을 계산한다.  

4. 손실(loss 또는 cost)값을 계산한다.  
   손실값이란 학습 데이터에 있는 목표 출력값으로부터 현재 계산된 값이 얼마나 다른지를 의미한다.  

5. 모델 파라미터를 조절한다.  
   실제적인 학습이 수행되는 단계이다.  
   손실값을 최소화하기 위해서 파라미터를 수정한다.   
   일반적으로 기울기 하강 알고리즘을 사용한다.(gradient descent algorithm)  
   
이러한 학습 루프는 학습률(training rate), 사용하는 모델과 입력 데이터에 따라서 여러 번 반복 수행된다.    

학습 후에는 평가 단계가 있다. 학습을 넘어서 얼마나 잘 예측하는지 판별하는 작업이다.   
보통 초기 데이터셋의 70%를 학습에, 나머지 30%를 평가에 사용한다.  


***

#### 선형 회귀(Linear Regression) // 연속적인 값(실수 값)을 예측하는 모델  

선형 회귀는 지도 학습 문제를 모델링하는 가장 단순한 형식이다. 학습 데이터로써 데이터 집합이 주어진다면,  
그 학습 데이터에 가장 잘 맞는 선형 함수(linear function)를 찾을 수 있다.  
2차원 데이터 집합에서 그러한 함수는 직선으로 표현된다.  

H(x) = Wx + b 로 표현할 수 있는데  
H(x)는 Hypothesis, x는 새로운 값을 예측하기 위해서 모델을 사용할 때 제공하는 값이다.  
W는 Weight, b는 bias를 의미한다.  W와 b가 모델의 파라미터이다.  


가장 중요한 것은 손실을 계산할 방법을 정의하는 것이다.  

간단한 모델에서 우리는 각 학습 샘플의 목표값(실제값)과 예측값의 차이의 제곱, 즉 제곱 에러(squared error)를 사용한다.  
제곱을 하는 이유는 제곱근 계산을 피할 수 있고, 목표값과 에측값의 차이가 음수가 나오는 것을 막을 수 있어서 이다.  

***

#### 로지스틱 회귀(Logistic Regression) // Yes,NO 또는 Pass,Non_Pass 형태(둘 중 하나)를 예측하는 모델  

선형 회귀 모델이 연속적인 값 또는 실수 값을 예측했다면, 이 "이메일이 스팸입니까? 와 같은 yes-no 형태의 질문에 답할 수 있는 모델이  
로지스틱 회귀 모델이다.  

머신 러닝에서 자주 사용되는 로지스틱 함수(logistic function)가 있다. 이 함수는 모양이 S(시그마)이기 때문에   
시그모이드(sigmoid)함수라고 한다.  

##### 로지스틱 함수는 특정한 입력값이 주어지면 출력값이 "참"일 확률을 계산하는 확률 분포 함수이다.  
즉, 질문에 대한 대답이 "YES"일 확률을 출력한다.  


이제 로지스틱 회귀 모델을 위한 손실 함수를 생각해보자.  

우리는 제곱 에러를 사용할 수 있었다.   
로지스틱 함수는 질문에 대한 대답으로 "YES"일 확률을 계산한다.  
학습 샘플에서 "YES" 또는 1의 출력값은 100%의 확률을 의미한다.  
여기서 손실값(제곱 에러 사용했을 때)은 그 샘플에 대하여 모델이 제시한 1보다 작은 값(확률값)의 제곱값을 의미한다.

예를 들어서 이 A라는 입력값 "YES"야? 라는 질문을 모델에 입력하였고, 모델의 출력값이 0.000000001 가 나왔다면,  
그  질문에 대한 대답이 "NO"일 가능성이 거의 100%라는 것을 의미한다.

이러한 문제에 더 잘 동작하는 크로스 엔트로피(cross entropy)라는 손실 함수가 있다.  
크로스 엔트로피는 출력값이 원하는 값보다 훨씬 멀기 때문에 훨씬 큰 값을 출력한다.  

크로스 엔트로피함수는 예상 확률값이 "YES"인 예제에 대해서는 출력값이 무한대에 가까이 증가한다.   


_제곱 에러에서는 예상 확률값이 "YES"일 때 1을 출력하지만, 크로스 엔트로피는 무한대에 가까운 값을 출력한다._


이것은 모델이 학습 이후에 잘못된 예측을 하는것을 불가능하게 만든다.  
이러한 점이 크로스 엔트로피가 로지스틱 회귀 모델을 위한 손실 함수로써 더 적당한 이유가 된다.  

***


