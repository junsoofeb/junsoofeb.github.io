---
bg: "deep.jpg"
layout: post
title:  "(인공)신경망"
crawlertitle: "Artificial Neural Networks"
summary: "ANN"
date:   2019-02-03
categories: posts
tags: 'deep_learning'
author: parkjunsoo
---

#### (인공)신경망 // 다층 퍼셉트론(multi-layer perceptron)



![ANN](https://github.com/junsoofeb/junsoofeb.github.io/raw/master/assets/images/ANN.png)

이렇게 신호를 주고 받는 모습이 네트워크(networks)이다.

여기에서 가장 왼쪽 줄을 입력층, 맨 오른쪽 줄을 출력층, 가운데 줄을 은닉층이라고 한다.

은닉층의 뉴런(노드)은 사람 눈에는 보이지 않는다.

또한, 일반적으로 입력층부터 출력층 방향으로 차례로 0층, 1층, 2층, 3층 ---이라고 한다.

층 수만 보면 3개의 층이지만, 가중치를 갖는 층은 2개이므로 2층 신경망이다.(문헌에 따라 3층이라고 하기도 함.)
##### 퍼셉트론 하나로 해결되지 않던 XOR 문제를 은닉층을 만들어 해결할 수 있었다.
##### 은닉층을 여러 개 쌓아올려 복잡한 문제를 푸는 과정이 사람의 신경망을 닮았다고 하여 인공 신경망이라 부르기 시작했고, 간단히 신경망이라고 통칭한다.
***

#### 활성화 함수(activation function)

입력신호의 총합을 출력신호로 변환하는 함수가 활성화 함수다.
활성화 함수는 입력신호의 총 합이 활성화(결과로 1을 출력)를 일으키는지 결정한다.

##### 일반적으로 단층 퍼셉트론은 계단 함수(임계값을 경계로 출력이 바뀌는 함수)를 활성화 함수로 사용한 모델을 가리키고, 다층 퍼셉트론은 신경망을 가리킨다.

##### 신경망은 여러 층으로 구성되며 시그모이드 함수, 렐루 함수 등 매끈한 모양의 활성화 함수를 사용한 모델이다.

#### 1. 시그모이드(sigmoid) 함수와 계단(step) 함수 비교


[공통점]
1) 둘 다 0에서 1사이의 값을 반환.
2) 둘 다 비선형 함수이다.

[차이점]
1) 시그모이드 함수는 0에서 1까지의 실수값(연속)을 반환하지만, 계단 함수는 0 또는 1(이산)만을 반환한다.
2) 외형적으로 시그모이드 함수는 곡선(매끈한 모양)이지만, 계단 함수는 직각의 계단 모양이다.

#### 2. ReLU 함수

Rectified Linear Unit의 줄임말로 rectify는 바로잡다, 정류하다 라는 의미다.
정류한다 라는 것은 -(마이너스)를 차단한다는 것이다.

실제로 렐루 함수는 입력이 0을 넘지 않으면(마이너스라면) 0을 출력하고,
입력이 0을 넘으면 입력값을 그대로 출력한다.

***
#### 출력층의 활성화 함수

기계 학습의 문제 유형은 크게 분류(classification)와 회귀(regression)으로 나뉜다.

분류 : 데이터가 어느 클래스에 속하는지 예측
회귀 : 입력 데이터에서 연속적인 수치를 예측

신경망은 분류와 회귀 모두에 이용할 수 있다.
다만, 둘 중 어느 것인가에 따라서 출력층에서 사용하는 활성화 함수가 달라진다.

##### 보통 출력층의 활성화 함수로, 2클래스 분류에는 sigmoid함수를 이용하고,
##### 다중 클래스 분류에는 softmax함수, 회귀에는 identity(항등) 함수를 사용한다.

#### 소프트맥스(softmax) 함수

![softmax](https://github.com/junsoofeb/junsoofeb.github.io/raw/master/assets/images/softmax.jpg)

_exp(x)는 자연상수에 지수가 x인 수식_

소프트맥스 함수는 지수함수이기 때문에 값이 빠르게 증가한다.
때문에 컴퓨터의 값 표현범위를 넘어갈 수 있다. (overflow 문제)

[overflow문제 해결방법]  

소프트맥스 함수 식에서 C라는 임의의 정수를 분모, 분자 양쪽에 곱한 후, exp() 안으로 옮겨 ln을 취해준다.  
마지막으로 ln(C)를 C'이라는 새로운 기호로 바꾼다.

##### C'을 사용한 식의 의미

 소프트맥스의 지수에 어떤 정수를 더하거나 빼도 결과는 변하지 않는다는 것.

C'은 어떤 값도 상관없지만, 보통 입력 신호 중에서 최댓값을 이용한다.

##### softmax함수의 특징

소프트맥스 함수의 출력은 0에서 1.0사이의 실수이다.  
소프트맥스 함수 출력의 총 합은 1이 된다.

이러한 성질 덕분에 소프트맥스 함수의 출력을 [확률]로 해석할 수 있다.

#### 출력층에서의 소프트맥스 함수

소프트맥스 함수를 사용해도 각 원소의 대소 관계는 변하지 않는다.  
y = exp(x)가 단조 증가 함수이기 때문이다.

신경망의 분류에서는 가장 큰 출력을 내는 뉴런에 해당하는 클래스로 구분하게 된다.

그런데 소프트맥스 함수를 적용해도 가장 큰 출력을 내는 노드가 바뀌지 않는다.

그래서 보통 신경망으로 분류할 때는 출력층의 소프트맥스 함수를 생략해도 된다.

##### 기계학습의 문제 풀이는 학습과 추론. 2 단계로 이루어진다.
##### 신경망을 학습시킬 때는 출력층에서 소프트맥스 함수를 사용하고,
##### 추론단계에서는 소프트맥스 함수를 생략하는 것이 일반적이다.
***

#### 출력층의 뉴런 수, 전처리, 정규화, 백색화

출력층의 뉴런의 개수는 풀려는 문제에 따라서 결정한다.  
분류에서는 분류하고 싶은 클래스 수만큼 뉴런 수를 설정한다.

전처리 : 신경망의 입력데이터에 특정한 일을 하는 것.

정규화 : 데이터를 특정 범위로 변환하는 것.

백색화 : 전체 데이터를 균일하게 분포시키는 것.

***

#### 배치(batch) 처리

batch는 일괄이라는 뜻이다.  

컴퓨터는 큰 배열 1개를 계산하는 것이 분할된 작은 배열들을 여러번 계산하는 것보다 빠르다.

배치 처리는 입력 데이터를 하나로 묶어서 처리(계산)하겠다는 것이다.  

배치 처리를 하면 데이터 1개당 처리시간이 많이 줄어든다.
